dpo:
  beta: 0.1
  learning_rate: 5.0e-7
  epochs: 3
  batch_size: 32 # Per device or effective
  gradient_accumulation_steps: 16
  preference_pairs: 15000
  on_policy_mix_ratio: 0.5
  score_gap_threshold: 2.0
  enable_iterative_refinement: true
